---
title: "Improving Image Captioning on Asian Cultural Dresses"
excerpt: "Fine-tuning BLIP for culturally specific image captioning<br/><img src='/images/image-captioning/poster.png' width='500' height='300'>"
collection: portfolio
---

**Improving Image Captioning on Asian Cultural Dresses** is a research project focused on enhancing caption generation for datasets underrepresented in vision-language tasks.  
By fine-tuning the **BLIP** model on a custom dataset of Asian cultural apparel, the project addressed bias and improved semantic relevance in generated captions.

### Key Features
- **Model Adaptation:** Fine-tuned BLIP with Hugging Face Transformers for cultural dress datasets.  
- **Performance Metrics:** Achieved BLEU score 0.218, METEOR 0.60, and ROUGE-L F1 0.418.  
- **Bias Mitigation:** Used targeted data augmentation to improve fairness and generalization.  
- **Research Impact:** Demonstrated the importance of culturally diverse datasets in multimodal AI.  

**Tools/Stack:** Python · PyTorch · BLIP · Hugging Face Transformers · CLIP · Jupyter  


---

🔗 [GitHub Repository](https://github.com/moazzamumer/Image-Captioning-Research)

---

## Screenshots
<img src="/images/image-captioning/result-1.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*

<img src="/images/image-captioning/result-2.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*

<img src="/images/image-captioning/result-3.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*

<img src="/images/image-captioning/result-4.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*

<img src="/images/image-captioning/result-5.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*

<img src="/images/image-captioning/result-6.jpg" width="500"><br/>
*Generated captions on Asian dress dataset*